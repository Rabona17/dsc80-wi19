{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 80: Homework 06\n",
    "\n",
    "### Due Date: Monday, Feb 18 12:00PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Much like in DSC 10, this Jupyter Notebook contains the statements of the homework problems and provides code and markdown cells to display your answers to the problems. Unlike DSC 10, the notebook is *only* for displaying a readable version of your final answers. The coding work will be developed in an accompanying `hw0X.py` file, that will be imported into the current notebook. (`X` is a homework number)\n",
    "\n",
    "Homeworks and programming assignments will be graded in (at most) two ways:\n",
    "1. The functions and classes in the accompanying python file will be tested (a la DSC 20),\n",
    "2. The notebook will be graded (for graphs and free response questions).\n",
    "\n",
    "\n",
    "**Do not change the function names in the `*.py` file**\n",
    "- The functions in the `*.py` file are how your assignment is graded, and they are graded by their name. The dictionary at the end of the file (`GRADED FUNCTIONS`) contains the \"grading list\". The final function in the file allows your doctests to check that all the necessary functions exist.\n",
    "- If you changed something you weren't supposed to, just use git to revert!\n",
    "\n",
    "**Tips for working in the Notebook**:\n",
    "- The notebooks serve to present you the questions and give you a place to present your results for later review.\n",
    "- The notebook on *HW assignments* are not graded (only the `.py` file).\n",
    "- Notebooks for PAs will serve as a final report for the assignment, and contain conclusions and answers to open ended questions that are graded.\n",
    "- The notebook serves as a nice environment for 'pre-development' and experimentation before designing your function in your `.py` file.\n",
    "\n",
    "**Tips for developing in the .py file**:\n",
    "- Do not change the function names in the starter code; grading is done using these function names.\n",
    "- Do not change the docstrings in the functions. These are there to tell you if your work is on the right track!\n",
    "- You are encouraged to write your own additional functions to solve the HW! \n",
    "    - Developing in python usually consists of larger files, with many short functions.\n",
    "    - You may write your other functions in an additional `.py` file that you import in `hw0X.py` (much like we do in the notebook).\n",
    "- Always document your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hw06 as hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic HTML tags practice\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "Create a very basic `html` file that satisfies the following properties:\n",
    "\n",
    "1. Has `<head>` and `<body>` tags.\n",
    "2. Has a title\n",
    "3. Inside the body tags:\n",
    "    * At least two headers\n",
    "    * At least three images:\n",
    "        * At least one image must be a local file\n",
    "        * At least one image must be linked to on-line source\n",
    "    * At least three references\n",
    "    * At least one table with two columns.\n",
    "   \n",
    "4. Save your work as `hw06_1.html` in the same directory as `hw06.py`, make sure it loads in the browser and do not forget to submit it.\n",
    "5. Do not forget to submit all data files needed to display your page. \n",
    "\n",
    "**Note:** You can toy with (basic) HTML in the cells of a notebook, using either a \"markdown cell\" or by using the `IPython.display.HTML` function. However, be sure to open your saved file in a browser to be sure the page displays properly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I'm ready for scraping! But am I allowed to?\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "We know that many sites have a published policy allowing or disallowing automatic access to their site. Often, this policy is in a text file `robots.txt`. There is (`https://moz.com/learn/seo/robotstxt`) a good article that explains what these files are, where to find them, and how to use them. After reading the article please answer a few questions. \n",
    "\n",
    "**2.1: What is the purpose of `robots.txt`?**\n",
    "\n",
    "1) To informs agents which pages to crawl.\n",
    "\n",
    "2) To informs agents that the site is automated.\n",
    "\n",
    "3) To inform agents that robots will chase them down if their info is stolen.\n",
    "\n",
    "**2.2: Where do you put your `robots.txt` file?**\n",
    "\n",
    "1) In the folder you want to disallow.\n",
    "\n",
    "2) In the root directory of your website.\n",
    "\n",
    "3) In a Google search.\n",
    "\n",
    "\n",
    "**2.3: If a `robots.txt` is not present, does it mean you can safely scrape the site?**\n",
    "\n",
    "1) Yes\n",
    "\n",
    "2) No\n",
    "\n",
    "**2.4: Each subdomain on a root domain can use separate `robots.txt` file**\n",
    "\n",
    "1) Yes\n",
    "\n",
    "2) No\n",
    "\n",
    "\n",
    "**2.5: Website hunt**\n",
    "\n",
    "Next, find three websites that explicitly use a `robots.txt` file and allow scraping and three that do not allow it.\n",
    "\n",
    "\n",
    "Now combine you answers to multiple choice questions in one list and urls of the sites you found in another list. \n",
    "Create an argument-free function `answers` to return both of lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count counties that start with different letters.\n",
    "\n",
    "**Question 3**\n",
    "\n",
    "In this question you need to scrape the website `http://example.webscraping.com/` and collect the data on all countries (not just the first page). I recommend you to check the website first, count the number of pages you need to go over, look at the URL of each page etc. Make sure to check the source page of the website in order to find appropriate tags for you to use during the scraping.\n",
    "\n",
    "Once you have an idea how the site works, you can start sending requests to the pages and collect results. \n",
    "\n",
    "*Side note:* We saw in the lecture that there is a method `pd.read_html` that allows you to read HTML tables into a list of DataFrame objects. You can test it out but please **DO NOT** use it in your solution. The purpose of this problem is for you to practice scraping using simple tags first, before you move on to more difficult problems. \n",
    "\n",
    "\n",
    "1. Create a function `find_countries` that takes in an url and return a dataframe with a single column that lists all countries from the given site. \n",
    "\n",
    "2. Create another function `first_letters_count` that takes in a dataframe with the list of countries and return another dataframe, indexed by a capital letter and the column containing the number of countries starting from this letter. That is, X countries start with an \"A\", Y countries start with a \"B\" etc.  Sort the column in decreasing order (by value). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: You must uncomment the doctests for this question to run them.** Please use the doctests for this question sparingly; every time you run them, it will scrape this page multiple times. \n",
    "1. Uncomment them only when you think you are done with the question,\n",
    "2. Replace the comments when you work further on other questions,\n",
    "3. Uncomment them lastly when you submit to gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://example.webscraping.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping an Online Bookstore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "\n",
    "Browse through the following fake on-line bookstore: http://books.toscrape.com/. This website is meant for toying with scraping.\n",
    "\n",
    "Scrape the website, collecting data on all books that have *at least* a four-star rating and are *under* £20. You should collect the data in a dataframe as below:\n",
    "\n",
    "<img src=\"bookdata.png\">\n",
    "\n",
    "Do this using the following steps:\n",
    "1. Create a function `extract_book_links` that takes in the content of a book-listing page (a string of html) and returns a list of urls of book-detail pages from which to scrape data.\n",
    "2. Create a function `get_product_info` that takes in the content of a book-detail page (a string of html) and returns a dictionary corresponding to a row in the dataframe in the image above.\n",
    "3. Create a function `scrape_books` of a single variable `k` that scrapes the first `k` pages of the bookstore (as determined by starting at the url above and clicking on the 'next' button), and returns a dataframe of books as described above.\n",
    "\n",
    "*Note:* Your function should take under 60 seconds to run through the entire bookstore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment Threads\n",
    "\n",
    "Analyzing conversations in on-line communities is a popular way to study human interactions. One incarnation of these conversations are found in comment threads. These threads consist of back-and-forth interactions between users. These conversations are structured as *trees*, consisting of posts and their replies (this is likely, at least in part, a consequence of being structured as HTML, which is tree-based). \n",
    "\n",
    "To analyze these conversations efficiently, it's often needed to translate them to a *tabular format*. To do this requires making some design choices, as trees are naturally a *nested* data structure, whereas tables are *flat*. \n",
    "\n",
    "Oftentimes you will find already flattened data in csv formats (e.g. when someone else does the scraping). For example, a comment thread that originally looked like this:\n",
    "\n",
    "---\n",
    "* post_id1 (user1) text1\n",
    "    * post_id2 (user2) reply_text2\n",
    "    * post_id3 (user3) reply_text3\n",
    "* post_id4 (user2) text4\n",
    "    * post_id5 (user4) reply_text5\n",
    "---\n",
    "\n",
    "Is stored in CSV format like this:\n",
    "\n",
    "|post_id|reply_to|user|text|\n",
    "|---|---|---|---|\n",
    "|post_id1|`NaN`|user1|text1|\n",
    "|post_id2|post_id1|user2|reply_text2|\n",
    "|post_id3|post_id1|user3|reply_text3|\n",
    "|post_id4|`NaN`|user2|text4|\n",
    "|post_id5|post_id4|user4|reply_text5|\n",
    "\n",
    "\n",
    "When you have naturally tree-structured data stored in tabular format, you often need to partially 'undo' the above flattening transformation to answer questions about the data. For example, you may ask questions like:\n",
    "1. How engaging is a given comment? \n",
    "2. How many different people are actively engaged in a comment thread?\n",
    "3. Is a flame-war occurring in a given comment thread?\n",
    "\n",
    "**Question 5**\n",
    "\n",
    "Assuming you are given comment threads stored in DataFrames as specified above, you want to calculate the following features (reflecting the above questions):\n",
    "\n",
    "1. For each comment, how many replies is it from a top-level post? (i.e. what is its depth?).\n",
    "    * Create a function `depth` that takes in a dataframe like the one above and returns a series of depths.\n",
    "2. For each comment, how many comments has it generated in reply?\n",
    "    * Create a function `descendants` that takes in a dataframe like the one above and returns the total number of comments that each comment has generated in reply (this includes replies of the replies).\n",
    "3. For each comment, how many distinct users have responded to it?\n",
    "    * Create a function `distinct_descendants` that takes in a dataframe like the one above and returns the total number of distinct users that have replied to each comment.\n",
    "\n",
    "From these attributes, you can analyze interesting statistics on the interactions between people on a comment thread. Interesting observations can also come out of comparing these attributes to the content of the comment text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to sample data\n",
    "comments_path = os.path.join('data', 'comments.csv')\n",
    "comments = pd.read_csv(comments_path, sep='|')\n",
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Congratulations, you're done with the homework***\n",
    "\n",
    "Now, run your doctests and upload hw06.py to GradeScope.¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
