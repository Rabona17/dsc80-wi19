{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 80: Homework 05\n",
    "\n",
    "### Due Date: Monday, Feb 11 12:00PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Much like in DSC 10, this Jupyter Notebook contains the statements of the homework problems and provides code and markdown cells to display your answers to the problems. Unlike DSC 10, the notebook is *only* for displaying a readable version of your final answers. The coding work will be developed in an accompanying `hw0X.py` file, that will be imported into the current notebook. (`X` is a homework number)\n",
    "\n",
    "Homeworks and programming assignments will be graded in (at most) two ways:\n",
    "1. The functions and classes in the accompanying python file will be tested (a la DSC 20),\n",
    "2. The notebook will be graded (for graphs and free response questions).\n",
    "\n",
    "\n",
    "**Do not change the function names in the `*.py` file**\n",
    "- The functions in the `*.py` file are how your assignment is graded, and they are graded by their name. The dictionary at the end of the file (`GRADED FUNCTIONS`) contains the \"grading list\". The final function in the file allows your doctests to check that all the necessary functions exist.\n",
    "- If you changed something you weren't supposed to, just use git to revert!\n",
    "\n",
    "**Tips for working in the Notebook**:\n",
    "- The notebooks serve to present you the questions and give you a place to present your results for later review.\n",
    "- The notebook on *HW assignments* are not graded (only the `.py` file).\n",
    "- Notebooks for PAs will serve as a final report for the assignment, and contain conclusions and answers to open ended questions that are graded.\n",
    "- The notebook serves as a nice environment for 'pre-development' and experimentation before designing your function in your `.py` file.\n",
    "\n",
    "**Tips for developing in the .py file**:\n",
    "- Do not change the function names in the starter code; grading is done using these function names.\n",
    "- Do not change the docstrings in the functions. These are there to tell you if your work is on the right track!\n",
    "- You are encouraged to write your own additional functions to solve the HW! \n",
    "    - Developing in python usually consists of larger files, with many short functions.\n",
    "    - You may write your other functions in an additional `.py` file that you import in `hw0X.py` (much like we do in the notebook).\n",
    "- Always document your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hw05 as hw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Data Transformation: log vs square root\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "A data transformation is the application of a deterministic mathematical function to each point in a data set. Mathematically, each data point $x_i$ is replaced with the transformed value $y_i = f(x_i)$, where $f$ is a transformation function. It is not easy to select a good transformation for a given prediction problem. There are many transformations to choose from and each has a different mathematical intuition. \n",
    "\n",
    "Generally, the goal of a transformation is to change the data into a *linear* relationship. Linear relationships are very easy to understand and one can then apply linear regression models.\n",
    "\n",
    "You've see an example of the log transformation in the lecture slides. Another popular transformation is a square root transformation (that is `f` is a square root function).\n",
    "\n",
    "In this problem you need to decide what transformation can be applied to a given dataset in order to make the relationship as *linear as possible* \n",
    "\n",
    "\n",
    "* To practice: create a dataframe of length at least 100 that contains the powers of two and plot the distribution on the histogram. Then apply the square root transformation, add another column to the original dataframe and plot another histogram. What change do you observe? Also use .plot to confirm your observation. Then plot these graphs side by side (.subplots might be handy here).  \n",
    "\n",
    "* Now repeat exactly the same steps but this time create a dataframe with an exponential distribution by raising the numbers from 1 to 99 to the value `e`, which is the base of the natural logarithm. Also plot this distribution, perform a log transformation and plot the results, as above. What did you observe?\n",
    "\n",
    "* Let's apply these ideas to the real dataset (`passengers.csv` in the data directory). You are given the Airline Passengers dataset with 144 monthly observations from 1949 to 1960. It includes a count of the number of airline passengers in thousands. You need to decide what transformation works better for this dataset: square or a log transformation. \n",
    "\n",
    "Create a function `best_transformation` that returns an integer with the value corresponding to the following choices:\n",
    "\n",
    "1. Square root transformation.\n",
    "2. Log transformation\n",
    "3. Both work the same.\n",
    "4. Neither gives a transformation revealing a linear relationship. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 : Data Transformation, reading results\n",
    "\n",
    "We are going to consider the growth in membership in a secret but very cool data science community. The dataset below shows the population (of data scientists) growth over ~200 days. The time is shown in days elapsed since the community was created. Over this period, the community existed \"under the radar\", i.e. not many people knew about it. You can think of this growth as a natural growth: while some people may leave the community to follow another path, others joined by word of mouth. So we can consider the rate of new converts fairly similar from day to day.\n",
    "\n",
    "\n",
    "You need to predict the amount of people in a community at 400 day mark. You will use a linear regression to make such a prediction. \n",
    "\n",
    "* First of all you need to plot the population by day and observe the curve. \n",
    "\n",
    "* Fit the regression line to these points; what are the slope, intercept, and R^2 coefficient? Either create these values using `numpy` functions alone or use `linregress` in `scipy.stats`. You may find a reminder from [DSC 10](https://www.inferentialthinking.com/chapters/15/2/Regression_Line) helpful!\n",
    "\n",
    "* Plot the line of best-fit, as well as the residual plot. You may find seaborns `regplot` useful!\n",
    "\n",
    "* Your next step is to try to apply two transformations to your data: square and a log transformations. After applying each transformation, fit the regression line to both plots. Which transformation give a better linear fit?  Compare the R^2 values for all three regression lines. Which model is better? Based on your choice, you need to predict the amount of people at 400 day mark.\n",
    "\n",
    "Write a function `prediction` that takes in a dataframe like `population` and returns your prediction and an integer answer choice (in this order) as a tuple.\n",
    "\n",
    "*Options:*\n",
    "1. My prediction is consistent with the population growths.\n",
    "2. My prediction is an underestimate\n",
    "3. My prediction is an overestimate because no data was given in this range. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating your own GeoJson\n",
    "\n",
    "In the next problem, you will create your own route on a map in a browser, create a GeoJson file from it, and plot it on a map using folium. There is a sample data file `data/aarons-day.txt` that maps out Aaron's daily routine to use as a sample file, as well.\n",
    "\n",
    "To create your own route and save it in a csv file:\n",
    "1. Go to [gmap-pedometer](https://www.gmap-pedometer.com/) and create your own route.\n",
    "2. To save the route, you need to open-up secret javascipt on the webpage. This is an example of scraping *hidden* material that requires giving an instruction to the page via javascript.\n",
    "    * *While on the page*, open up the *developer console*. In Chrome: `View -> Developer -> Developer Console`.\n",
    "    * This console looks a little like a Python console. Paste in the following function (all one line):\n",
    "    ```\n",
    "    (function(){var script=document.createElement('script');script.src='gmapToGpx.min.js';document.getElementsByTagName('head')[0].appendChild(script);})()\n",
    "    ```\n",
    "    * A window with GPX data should open up at the top of the page. It starts with `<?xml=...`. You should copy this text (see screenshot below).\n",
    "3. Go the following [GPX conversion website](http://www.gpsvisualizer.com/convert_input). Paste in your GPX data and press 'convert'. (All the default options should be selected). That text file should look like `data/aarons-day.txt`.\n",
    "\n",
    "**Screenshot of saving a route:**\n",
    "\n",
    "<img src=route2gpx.png>\n",
    "\n",
    "\n",
    "**Question 3**\n",
    "\n",
    "Now you should create a function `latlong2geojson` which takes in a dataframe like `route` and outputs a dictionary representing the GeoJson of your route. You should add at least two descriptive attributes of stops/landmarks along the way (displayed as markers).\n",
    "\n",
    "*Note:* There are resources online that would largely do this problem for you; avoid them! Outside of `pandas` (and `folium` to plot it) do *not* use any additional libraries. This is meant to familiarize you with the basics of what makes up a geojson file.\n",
    "\n",
    "*Note:* The attributes are different features (i.e. locations) than the route itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join('data', 'aarons-day.txt')\n",
    "route = pd.read_csv(path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPS Mobile Data\n",
    "\n",
    "Microsoft Research Asia conducted a study with samples of mobile GPS trajectories. This data is available to the public. The study has GPS tracking of 185 people, varying from a few minutes worth of tracking to multiple hours over many days. One of these trajectories is found the file `20081023025304.plt` in the data directory.\n",
    "\n",
    "The file format is described below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 4.1. Trajectory file\n",
    "* Every single folder of this dataset stores a user’s GPS log files, which were converted to PLT format. \n",
    "* Each PLT file contains a single trajectory and is named by its starting time. \n",
    "* To avoid potential confusion of time zone, we use GMT in the date/time property of each point, which is different from our previous release.\n",
    "\n",
    "> PLT format:\n",
    "* Line 1…6 are useless in this dataset, and can be ignored. Points are described in following lines, one for each line.\n",
    "* Field 1: Latitude in decimal degrees.\n",
    "* Field 2: Longitude in decimal degrees.\n",
    "* Field 3: All set to 0 for this dataset.\n",
    "* Field 4: Altitude in feet (-777 if not valid).\n",
    "* Field 5: Date - number of days (with fractional part) that have passed since 12/30/1899.\n",
    "* Field 6: Date as a string.\n",
    "* Field 7: Time as a string.\n",
    "\n",
    "> Note that field 5 and field 6&7 represent the same date/time in this dataset. You may use either of them.\n",
    "Example:\n",
    "* 39.906631,116.385564,0,492,40097.5864583333,2009-10-11,14:04:30\n",
    "* 39.906554,116.385625,0,492,40097.5865162037,2009-10-11,14:04:35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "\n",
    "Create a function `trajectory_distance` that takes in a *file path* of a `plt` file and returns the distance of the trajectory *in miles*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinates and Polygons\n",
    "\n",
    "In this question, you will create a function that labels the state in which a lat/long pair lies. To do this, use the following steps:\n",
    "* Use the coordinate polygons in the GeoJson file `states.geojson` (in the data directory) to define lat-long regions for each state.\n",
    "* Define a `matplotlib.path.Path` object using the states file and use the method `contains_point` to determine is a given point is in a given state.\n",
    "\n",
    "More precisely:\n",
    "* Define a function `point_in_state` which takes in \n",
    "    - a lat-long coordinate pair, \n",
    "    - a state abbreviation, and \n",
    "    - a geojson dictionary\n",
    "\n",
    "and returns `True`/`False` if the coordinate lies in that state.\n",
    "\n",
    "* Define a function `label_state` which takes in a lat-long pair and the GeoJson dictionary, and returns the state in which the coordinate lies. If the coordinate isn't in any state, it should return `None`.\n",
    "\n",
    "*Hint:* Recall that GeoJson uses pairs of coordinates of the form (longitude, latitude). You can check your work by plotting the region and the point using folium!\n",
    "\n",
    "*Hint:* Take care of states that have more than one piece (e.g. HI, AK, MI)\n",
    "\n",
    "*Note:* You can the function `point_in_state` to estimate the relative area of each state in the US using the following Monte Carlo simulation: randomly draw a coordinates; keep a count of which draws lie in which states. This count defines an empirical distribution of relative areas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.path as mpltPath\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations, you're done with the homework\n",
    "\n",
    "### Now, run your doctests and upload `hw05.py` to GradeScope.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
