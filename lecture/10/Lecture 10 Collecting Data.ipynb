{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "* HTTP requests\n",
    "* API requests and tokens\n",
    "* Web Scraping and Crawling\n",
    "* Parsing HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Collecting Data\n",
    "\n",
    "* Often the data you need doesn't exist in 'clean' csv files\n",
    "* Solution: collect your own data!\n",
    "    - Design your own experiment and collection (e.g. surveys, experiments).\n",
    "    - Find related data on the internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data on the Internet\n",
    "\n",
    "* The Internet contains *large* amounts of historical record:\n",
    "    - archived record of events in the world (e.g. news stories)\n",
    "    - human behaviors on the Internet (e.g. social network behaviors)\n",
    "* Data for many natural experiments available for consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to collect data from the Internet\n",
    "\n",
    "* Use a \"data request endpoint\" (i.e. published API)\n",
    "* Scrape and crawl content from web-pages.\n",
    "\n",
    "Both communicate with content via HTTP requests!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HTTP\n",
    "\n",
    "* HyperText Transfer Protocol: is a *request-response* protocol \n",
    "* Allows one computer to talk to another over the Internet. \n",
    "* Used to fetch data from servers hosting web-content ('web servers').\n",
    "\n",
    "Understanding what it is and how the protocol works in the first step to understanding how things work on the Internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Request-response model\n",
    "\n",
    "HTTP follows the standard client - server model (or request - response). \n",
    "\n",
    "* Customer - Bank analogy\n",
    "* Web browser are `clients` to HTTP\n",
    "* Youtube, Facebook, etc are `servers` that are sitting somewhere else.\n",
    "\n",
    "<img src=\"HTTP-Protocol.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Request-response model\n",
    "\n",
    "When a request is sent to view content on a web-page:\n",
    "* The server must process your request (prepare data for response)\n",
    "* Send content from the the server to the client in its response.\n",
    "\n",
    "This process requires computational resources for the server!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Anecdote: Web-page hosting\n",
    "\n",
    "* 15 year old makes a web-page that allows a user to:\n",
    "    1. type in their name (client request)\n",
    "    2. and receive a picture their (name's) spirit animal (server response).\n",
    "* This requires a server-side 'look-up' of a picture for a given name.\n",
    "* Hit Reddit front-page; $15,000 bill from cloud provider!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Anecdote: Web-page hosting\n",
    "\n",
    "Moral: every time you view a page, it costs the page owner money!\n",
    "\n",
    "* Be mindful of your impact when collecting data from the Internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# HTTP Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Making HTTP requests using `curl`\n",
    "\n",
    "[`curl`](https://curl.haxx.se/docs/httpscripting.html) is a command-line tools that sends HTTP requests like a browser.\n",
    "\n",
    "1. The client, curl, sends a HTTP request. \n",
    "2. The request contains a method (e.g GET, POST, HEAD)\n",
    "3. The HTTP server responds with \n",
    "    - a status line (indicating if things went well), \n",
    "    - response headers \n",
    "    - a response body (usually) including the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Request methods\n",
    "\n",
    "* GET: is used to request data from a specified resource.\n",
    "\n",
    "* HEAD: is almost identical to GET, but without the response body. HEAD requests are useful for checking what a GET request will return before actually making a GET request - like before downloading a large file or response body.\n",
    "\n",
    "* DELETE: deletes the specified resource.\n",
    "\n",
    "* POST: is used to send data to the server, for example, customer information, file upload.\n",
    "\n",
    "* a few more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: GET\n",
    "\n",
    "* `curl` by default issues a `GET`:\n",
    "\n",
    "```\n",
    "curl -v https://httpbin.org/html \n",
    "```\n",
    "\n",
    "* `-v` is short for verbose\n",
    "* Now type the same URL in the browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running this in Windows won't work\n",
    "# curl -v https://httpbin.org/html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: POST\n",
    "\n",
    "* a POST request with `curl` that sends 'Humpty Dumpty' as the parameter 'name'.\n",
    "```\n",
    "curl -d 'name=Humpty Dumpty' https://httpbin.org/post\n",
    "```\n",
    "\n",
    "* `-d` is short for POST\n",
    "* You cannot send a POST directly in the url of a browser.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running this in Windows won't work\n",
    "# curl -d 'name=Humpty Dumpty' https://httpbin.org/post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GET vs POST\n",
    "\n",
    "* GET can also send data to the server. With the GET method, the data is appended to the URL with a special syntax:\n",
    "    * `http://cofe.ru/forum?id=666`\n",
    "    * This method is good when you're sending small amounts of data. \n",
    "    * Also there might be cases you want parameters to be 'visible' in the URL (e.g. for crawlers)\n",
    "    \n",
    "* In POST, the URL visible to the user is just `http://cofe.ru/forum`. \n",
    "    * and any parameter setting is \"hidden\" in the communication with the server. \n",
    "    * It is not encrypted, just hidden.\n",
    "\n",
    "* reference: https://www.w3schools.com/tags/ref_httpmethods.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Requests\n",
    "\n",
    "* `requests` is a python package that allows you to use Python to interact with the Internet!  \n",
    "* There are other packages (e.g. `urllib`), but `requests` is the easiest to use.\n",
    "\n",
    "In fact, to get the UCSD home page is a simple as\n",
    "```\n",
    "import requests\n",
    "text = requests.get(\"https://ucsd.edu\").text\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# GET request (default)\n",
    "url = \"https://ucsd.edu\"\n",
    "resp = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response object\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# body of response\n",
    "resp.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.request.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post data with in a dictionary (json!)\n",
    "post_response = requests.post(\"https://httpbin.org/post\",\n",
    "                              data={'name': 'Humpty Dumpty'})\n",
    "post_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_response.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(post_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Status Codes\n",
    "\n",
    "* When we request data from a website, the server responds with a HTTP status code.  \n",
    "* The most common response is `200` which means things went well.  \n",
    "* Other times you will get a different status code saying something else happened\n",
    "    - you might be familiar with a `404` which means the page wasn't found.\n",
    "\n",
    "* This great site lists http status codes: [https://httpstat.us/](https://httpstat.us/).\n",
    "\n",
    "* But better yet, it has example sites that return a certain code, so you can test!  \n",
    "    - So, for example, https://httpstat.us/404 returns a `404`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get(\"https://httpstat.us/404\")\n",
    "print(r.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://httpstat.us/404\")\n",
    "r.status_code\n",
    "r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check if the call went ok with `r.ok` which returns a boolean.\n",
    "\n",
    "After you run the code below, read up on each of the status codes at [https://httpstat.us/](https://httpstat.us/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statusCodes = [200, 404, 403, 429]\n",
    "\n",
    "for statusCode in statusCodes:\n",
    "    r = requests.get(\"https://httpstat.us/\" + str(statusCode))\n",
    "    print(str(statusCode) + \" ok: \" + str(r.ok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or raise an exception when there is a not-ok status code\n",
    "\n",
    "r = requests.get(\"https://httpstat.us/404\")\n",
    "r.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The data formats of the Internet\n",
    "\n",
    "Working with Internet communication, you will need to parse the data formats of the internet.\n",
    "* GET: response body is usually HTML\n",
    "* POST: response body is usually JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## JSON from the Internet\n",
    "\n",
    "* JSON is read as a python dictionary. How is it different?\n",
    "* Always use `json.loads` and ***never*** `eval` the text to a `dict`!\n",
    "* You generally cannot trust the data you receive from HTTP requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json stored as string\n",
    "post_response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALWAYS parse using json library\n",
    "import json\n",
    "json.loads(post_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malicious = open('response.json').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the text of the malicious request\n",
    "print(malicious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval it to a dictionary (BAD, DON'T DO THIS)\n",
    "# SERIOUSLY. DON'T. DO. IT.\n",
    "eval(malicious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse with JSON library (GOOD)\n",
    "json.loads(malicious)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Handling 'unfamiliar' data\n",
    "* Never trust data from an unfamiliar site\n",
    "* **Never** use `eval` on 'raw' data that you didn't create!\n",
    "* The json data format needs to be *parsed*, not evaluated as a dictionary.\n",
    "    - It was designed with safety in mind!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Getting data from the Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Programmatic requests\n",
    "\n",
    "* We learned how to use python `requests` to exchange data via HTTP.\n",
    "* There are two ways of collecting data with requests:\n",
    "    * Use a **published API** (Application Programming Interface)\n",
    "    * **'Screen Scrape'** a page to collect raw HTML as in a browser\n",
    "    \n",
    "* Takeaway: always use a published API whenever possible!\n",
    "\n",
    "**Important**: stay out of trouble!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## APIs vs Scraping\n",
    "\n",
    "- APIs allow for authentication (and access to sensitive data).\n",
    "- APIs have more reliable data that is easier to parse.\n",
    "- APIs allow hosts to monitor usage and protect their website\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## APIs vs Scraping\n",
    "    \n",
    "* Scraping mimics the browser and is uncontrolled like normal browsing.\n",
    "    - Many rapid requests to a small site can take down the host server.\n",
    "    - Many rapid requests to a small site can cost the owner money\n",
    "    - Always be respectful and responsible when scraping!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is an API?\n",
    "\n",
    "* APIs are url endpoints dedicated for programmatic requests setup by the website host.\n",
    "\n",
    "* **An endpoint** is a server route that is used to retrieve different data from the API. \n",
    "\n",
    "For example, on the Reddit API:\n",
    "* the `/comments` endpoint retrieves information about comments, \n",
    "* the `/users` endpoint might retrieve data about users. \n",
    "\n",
    "To access them, you would add the endpoint to the base url of the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why APIs are *always* better\n",
    "\n",
    "* The data is changing quickly. \n",
    "    - E.g. stock price data: don't want to scrape a page every few minutes!\n",
    "* You want a small piece of a much larger set of data. \n",
    "    - What if you want to just pull your own comments on Reddit? (all is too much)\n",
    "    - What if you want your Google GPS history? (private)\n",
    "* You want usability and stability, not changing HTML requiring translation.\n",
    "    - Websites change *all the time!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## API requests\n",
    "\n",
    "* API requests are just GET/POST requests to a specially maintained URL.\n",
    "* Below is `okpy` (DSC 10) grading API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a get request \n",
    "response = requests.get(\"https://okpy.org/api/v3/\")\n",
    "\n",
    "# Print the status code of the response. Are we in trouble?\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a get request \n",
    "response = requests.get(\"https://okpy.org/api/v3.json\")\n",
    "\n",
    "# Print the status code of the response. and now?\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a get request \n",
    "response = requests.get(\"https://okpy.org/api/v3/version/\")\n",
    "\n",
    "# Print the status code of the response. and now??\n",
    "print(response.status_code)\n",
    "print(response.content)\n",
    "\n",
    "# print(json.loads(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example API request\n",
    "\n",
    "* Submit and assignment\n",
    "\n",
    "<img src=\"okpy_request.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example API request\n",
    "\n",
    "* Submit a score after grading the HW\n",
    "* Notice the authentication token!\n",
    "\n",
    "<img src=\"okpy_get.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why Scraping?\n",
    "\n",
    "* Not all scraping is unethical!\n",
    "* Google scrapes pages to make them searchable\n",
    "* You may be able to *bring value* to the website you're scraping!\n",
    "* Making an API takes work -- scraping may be the only way!\n",
    "\n",
    "How do you scrape ethically and responsibly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Best practices for scraping\n",
    "\n",
    "1. Send requests slowly; be upfront about what you are doing!\n",
    "2. Respect the policy published in the page's `robots.txt` file\n",
    "3. Don't spoof your [UserAgent](https://en.wikipedia.org/wiki/User_agent) (or try to trick the server into thinking you are a person)\n",
    "4. Read the Terms of Service for the site and follow it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Consequences of irresponsible scraping\n",
    "\n",
    "* Too many requests may cause the server will block your IP Address!\n",
    "    - Everyone in your dorm might lose access to Google! (Seriously!)\n",
    "    - Bad enough? May bring legal trouble!\n",
    "* Too many requests may take down the website\n",
    "    - A scraper (journalist) took down the Cook County Inmate Locater\n",
    "    - Ethical implication: inmate's families couldn't contact them while the site was down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `Robots.txt`\n",
    "\n",
    "* Many sites have a published policy allowing or disallowing automatic access to their site.  \n",
    "* This policy is in a text file `robots.txt`: learn more about it [here](https://moz.com/learn/seo/robotstxt).\n",
    "* The code below checks if the `robot.txt` file prohibits you from scraping the site.  \n",
    "* Remember the best practices above - just because you aren't prohibited by the robots policy doesn't mean you can scrape the site!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import urllib.robotparser\n",
    "\n",
    "# This code checks the robots.txt file\n",
    "# include it in scraping scripts\n",
    "def canFetch(url):\n",
    "\n",
    "    parsed_uri = urlparse(url)\n",
    "    domain = '{uri.scheme}://{uri.netloc}/'.format(uri=parsed_uri)\n",
    "\n",
    "    rp = urllib.robotparser.RobotFileParser()\n",
    "    rp.set_url(domain + \"/robots.txt\")\n",
    "    try:\n",
    "        rp.read()\n",
    "        canFetchBool = rp.can_fetch(\"*\", url)\n",
    "    except:\n",
    "        canFetchBool = None\n",
    "    \n",
    "    return canFetchBool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://slate.com/bullpen/\"\n",
    "canFetch(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://dsc.ucsd.edu/node/10\"\n",
    "canFetch(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Should I use an API or build a scraper?\n",
    "\n",
    "Taken from [a journalist's talk](http://www.storybench.org/to-scrape-or-not-to-scrape-the-technical-and-ethical-challenges-of-collecting-data-off-the-web/)\n",
    "<img src=\"flowchart_final.jpeg\" width=\"75%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The result of scraping: HTML data\n",
    "\n",
    "* GET requests result in HTML; What does this look like?\n",
    "* Let's see what is on the UCSD Data Science Events page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://dsc.ucsd.edu/node/10\"\n",
    "\n",
    "r = requests.get(url)\n",
    "    \n",
    "urlText = r.text\n",
    "\n",
    "Nchars = 10000\n",
    "print(urlText[:Nchars]) # Print the first 500 characters\n",
    "print(\"\\n\\n... \" + str(len(urlText)-Nchars) + \" additional characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that is gross looking!  It is **raw** HTML, which the browser uses to make the viewable site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is HTML?\n",
    "\n",
    "* HTML (HyperText Markup Language) is the most basic building block of the Web. It describes and defines the content of a webpage along with the basic layout of the webpage.\n",
    "\n",
    "* HTML markup includes special \"elements\" (tags) such as \n",
    "    * `<head>, <title>, <body>, <p>, <div>, <img>`,.....\n",
    "    \n",
    "\n",
    "See [this tutorial](http://fab.academany.org/2018/labs/fablaboshanghai/students/bob-wu/Fabclass/week2_project_management/HTML.html) for more reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat lec10.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a file via python\n",
    "\n",
    "import webbrowser\n",
    "import os\n",
    "webbrowser.open('file://' + os.path.realpath(\"lec10.html\"));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Images and Hyperlinks\n",
    "\n",
    "* Tag for a picture (can use a link to the image):\n",
    "\n",
    "`<img src=\"HumDum.png\" alt=\"Humbpty Dumpty\">`\n",
    "\n",
    "* Tag for a hyperlink: \n",
    "\n",
    "  `<a href=\"https://ucsd.edu/\">Visit our page!</a>`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat lec10_pic_ref.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "import os\n",
    "webbrowser.open('file://' + os.path.realpath(\"lec10_pic_ref.html\"));\n",
    "\n",
    "# the size of the images can be edited. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cleaning\n",
    "\n",
    "* Now we have an idea what the basic structure of the HTML looks like, we can start cleaning it. \n",
    "\n",
    "* To process it we can use [Beautiful Soup 4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).\n",
    "\n",
    "**Warning:** BeautifulSoup has changed quite a bit between versions, so make sure you are looking at documentation for the version you are using (4 here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"http://dsc.ucsd.edu/node/10\"\n",
    "# r = requests.get(url)   \n",
    "# urlText = r.text\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(urlText, 'html.parser')\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can extract the title of the document\n",
    "\n",
    "soup.title\n",
    "# soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can extract the first paragraph \n",
    "\n",
    "soup.p\n",
    "\n",
    "# open link in the browser, right click and \"page source\". Can you find <p> tags?\n",
    "# and hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab all the links\n",
    "\n",
    "all_links  = soup.find_all('a')\n",
    "all_links\n",
    "#type(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all the links\n",
    "for link in soup.find_all('a'):\n",
    "    print(\"new link: \" + link.text+ \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in soup.find_all('a'):\n",
    "    print(\"new link: \" + str(link.get('href'))+ \"\\n\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the text\n",
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "livereveal": {
   "scroll": true,
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
