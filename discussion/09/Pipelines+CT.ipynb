{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Introduction to Pipelining\n",
    "- Utility within scikit-learn in Python\n",
    "- Extremely simple and useful tool for managing machine learning workflows\n",
    "- Usefulness of pipelines:\n",
    "    - Standardise the operations of your ML task\n",
    "    - Chain them in a sequence, make unions and finetune parameters\n",
    "    - Reproducibility\n",
    "    - Value in persistence of entire pipeline objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What does a typical ML task entail?\n",
    "\n",
    "- data preparation to varying degrees\n",
    "    - getting a cleaned dataset from an initial state of disarray (data cleaning, data wrangling)\n",
    "    - various pre-processing steps (dimensionality reduction, feature extraction)\n",
    "- finish off with a prediction or a modeling task "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The Pipeline class is a manageable way to apply a series of data transformations followed by the application. (i.e. your choice of ML model)\n",
    "\n",
    "To give a one line summary - `Pipeline of transforms with a final estimator.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Steps for modelling a Pipeline\n",
    "\n",
    "1. Feature Engineering - Create features to best reflect the meaning behind data\n",
    "2. Create an approriate model to capture relationships between features; e.g. linear, non-linear\n",
    "3. Select a loss function and fit the model\n",
    "4. Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Once you perform these steps, your pipeline is ready. \n",
    "Now you can use the model for prediction and/or inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How Pipelines help with Data Preparation \n",
    "\n",
    "- `Data Preparation` should ensure strong separation of training and testing data\n",
    "- A common problem in applied ML is overfitting which can ocur when data from your training set is leaked to your testing set\n",
    "\n",
    "eg. Data Prepartion using normalization or standardization on the entire training dataset before learning would not be a valid test, because the training dataset would have been influenced by the scale of the data in the test set.\n",
    "\n",
    "- Pipelines help you prevent data leakage\n",
    "- Ensure that the data preparation is constrained to each fold of your cross validation procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How Pipelines help with Feature Extraction and Modelling \n",
    "\n",
    "- `Feature extraction` is another procedure that is susceptible to data leakage\n",
    "- The pipeline provides a handy tool called the `FeatureUnion`\n",
    "    - Allows the results of multiple feature selection and extraction procedures to be combined\n",
    "    - Combined larger dataset can be used to train the model\n",
    "- Thus, all the feature extraction and the feature union occurs within each fold of the cross validation procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to put together a Scikit-Learn Pipelines\n",
    "- Put together feature transformers and models using sklearn.Pipeline objects\n",
    "- Create a pipeline: <i>pl = Pipeline([feat, mdl])</i>\n",
    "- Fit the model(s) in the pipeline using pl.fit(data, target)\n",
    "- Predict from raw input data through the pipeline using pl.predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simple Examples of a Pipeline\n",
    "\n",
    "#### Example 1 \n",
    "- We use the iris dataset\n",
    "- We perform pre-preprocessing by standardizing the data\n",
    "- We use a Logistic Regressor to classify the dataset into its target iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and split the data\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size= 0.2, random_state=42)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lr = Pipeline([('stdscr', StandardScaler()),\n",
    " ('clf', LogisticRegression(solver='newton-cg', multi_class='ovr'))])\n",
    "\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "# Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('stdscr', StandardScaler(copy=True, with_mean=True, with_std=True)), ('clf', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='newton-cg',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression pipeline test accuracy: 0.967\n"
     ]
    }
   ],
   "source": [
    "score = pipe_lr.score(X_test, y_test)\n",
    "print('Logistic Regression pipeline test accuracy: %.3f' % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simple Examples of a Pipeline\n",
    "\n",
    "#### Example 2 \n",
    "- We use the digits data from sk-learn \n",
    "- We perform pre-preprocessing by Principal Component Anaysis, where we choose the top 20 features\n",
    "- We use a Logistic Regressor using Stochastic Gradient Descent and early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=20, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)), ('logistic', SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=True, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.1...dom_state=0, shuffle=True, tol=1e-05,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "logistic = SGDClassifier(loss='log', penalty='l2', early_stopping=True,\\\n",
    "                         max_iter=10000, tol=1e-5, random_state=0)\n",
    "pca = PCA(n_components = 20)\n",
    "pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "pipe.fit(X_digits, y_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The above example was just to observe another kind of data transformation, and not necessarilly for a viable ML task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Function Transformer\n",
    "- Recall what a function transformer is\n",
    "- It forwards the X (and optionally y) arguments to a user-defined function or function object and returns the result of this function\n",
    "- Used in Data Pre-processing\n",
    "- Somewhat like an `apply` in pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But what if we cannot apply the same transformations to every individual feature of a data point in X?\n",
    "This is why we need `Column Transformers`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Column Transformer\n",
    "- Datasets can often contain components that require different feature extraction and processing pipelines\n",
    "- Datasets may have a mix of Categorical columns and Continuous Numeric columns, which will almost always need separate transformations\n",
    "- Datasets may be stored in a Pandas DataFrame and different columns require different processing pipelines\n",
    "\n",
    "For Example:\n",
    "    - Your dataset consists of heterogeneous data types (e.g. raster images and text captions)\n",
    "    - You want to standardize the numerical columns but one-hot-encode the categorical ones\n",
    "\n",
    "- The brand new ColumnTransformer allows you to choose which columns get which transformations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The ColumnTransformer takes a list of tuples, where each tuple has the following 3 entries:\n",
    "    - The first value in the tuple is a name that labels it, \n",
    "    - the second is an instantiated estimator or transformation, \n",
    "    - and the third is a list of columns you want to apply the transformation to. \n",
    "- The tuple will look like this:\n",
    "     `('name', SomeTransformer(parameters), columns)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example of a Pipeline using Column - Transformer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let's work on the titanic dataset from class\n",
    "- Import the data into  dataframe\n",
    "- To avoid complications and focus on column transformations, let's drop the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"https://raw.githubusercontent.com/amueller/scipy-2017-sklearn/master/notebooks/datasets/titanic3.csv\")\n",
    "\n",
    "titanic = titanic.dropna(subset=['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The target column is chosen as `survived` as done in the hw\n",
    "- The features we work with are added to `features` list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pclass     sex      age      fare embarked\n",
       "0       1  female  29.0000  211.3375        S\n",
       "1       1    male   0.9167  151.5500        S\n",
       "2       1  female   2.0000  151.5500        S\n",
       "3       1    male  30.0000  151.5500        S\n",
       "4       1  female  25.0000  151.5500        S"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = titanic.survived.values\n",
    "features = titanic[['pclass', 'sex', 'age', 'fare', 'embarked']]\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Now lets apply ColumnTransformer on different column features\n",
    "- Numeric features in consideration are `age` and `fare`\n",
    "- Categorical features in consideration `pclass`, `sex`, `embarked`\n",
    "- We standardize the numerical features, and one-hot-encode the categorical features as stated above\n",
    "- we use ColumnTranformer to respectively assign these transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "numeric_features = ['age', 'fare']\n",
    "categorical_features = ['embarked', 'sex', 'pclass']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Now we construct a pipeline using the transformer object\n",
    "- Finally we use the pipeline to fit and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression score: 0.804598\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=0)\n",
    "\n",
    "model = make_pipeline(\n",
    "    preprocessor,\n",
    "    LogisticRegression(solver='lbfgs'))\n",
    "model.fit(X_train, y_train)\n",
    "print(\"logistic regression score: %f\" % model.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [Anaconda3]",
   "language": "python",
   "name": "Python [Anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
